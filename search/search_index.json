{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Vq","text":"<p>Vq (vector quantizer) is a vector quantization library for Rust. It provides efficient implementations of popular quantization algorithms for compressing high-dimensional vectors.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Simple and generic API via the <code>Quantizer</code> trait</li> <li>Morer than 50% reduction in storage size of input vectors</li> <li>SIMD acceleration support (AVX/AVX2/AVX512/NEON/SVE) via the <code>simd</code> feature</li> <li>Multi-threaded training via the <code>parallel</code> feature</li> <li>Multiple distance metrics: Euclidean, Manhattan, Cosine</li> </ul>"},{"location":"#supported-algorithms","title":"Supported Algorithms","text":"Algorithm Training Quantization Compression Use Case Binary (BQ) \\(O(1)\\) \\(O(nd)\\) 75% Fast binary similarity Scalar (SQ) \\(O(1)\\) \\(O(nd)\\) 75% Uniform value ranges Product (PQ) \\(O(nkd)\\) \\(O(nd)\\) 50% Large-scale ANN search Tree-Structured (TSVQ) \\(O(n \\log k)\\) \\(O(d \\log k)\\) 50% Hierarchical clustering <p>Where \\(n\\) = number of vectors, \\(d\\) = dimensions, \\(k\\) = centroids.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>use vq::{BinaryQuantizer, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Create a binary quantizer with threshold 0.0\n    let bq = BinaryQuantizer::new(0.0, 0, 1)?;\n\n    // Quantize a vector\n    let quantized = bq.quantize(&amp;[-1.0, 0.5, 1.0])?;\n    assert_eq!(quantized, vec![0, 1, 1]);\n\n    Ok(())\n}\n</code></pre>"},{"location":"#python-bindings","title":"Python Bindings","text":"<p>Python bindings are available via PyVq:</p> <pre><code>pip install pyvq\n</code></pre> <p>See the PyVq documentation for Python-specific guides.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started</li> <li>API Reference (docs.rs)</li> <li>GitHub Repository</li> <li>PyPI Package</li> </ul>"},{"location":"api/distance/","title":"Distance API Reference","text":"<p>The <code>Distance</code> enum provides distance metric implementations for vector comparisons.</p>"},{"location":"api/distance/#enum-variants","title":"Enum Variants","text":"<pre><code>pub enum Distance {\n    /// Squared Euclidean distance (L2\u00b2)\n    SquaredEuclidean,\n\n    /// Euclidean distance (L2)\n    Euclidean,\n\n    /// Manhattan distance (L1)\n    Manhattan,\n\n    /// Cosine distance (1 - cosine similarity)\n    CosineDistance,\n}\n</code></pre>"},{"location":"api/distance/#methods","title":"Methods","text":""},{"location":"api/distance/#compute","title":"<code>compute</code>","text":"<pre><code>impl Distance {\n    pub fn compute(&amp;self, a: &amp;[f32], b: &amp;[f32]) -&gt; VqResult&lt;f32&gt;\n}\n</code></pre> <p>Computes the distance between two vectors.</p>"},{"location":"api/distance/#arguments","title":"Arguments","text":"<ul> <li><code>a: &amp;[f32]</code> - First vector</li> <li><code>b: &amp;[f32]</code> - Second vector</li> </ul>"},{"location":"api/distance/#returns","title":"Returns","text":"<ul> <li><code>Ok(f32)</code> - The computed distance</li> <li><code>Err(VqError::DimensionMismatch)</code> - If vectors have different lengths</li> </ul>"},{"location":"api/distance/#simd-acceleration","title":"SIMD Acceleration","text":"<p>When the <code>simd</code> feature is enabled, this method uses SIMD instructions where available:</p> <ul> <li>x86/x86_64: AVX, AVX2, AVX512, FMA</li> <li>ARM: NEON, SVE</li> </ul> <p>The backend is automatically selected at runtime.</p>"},{"location":"api/distance/#examples","title":"Examples","text":""},{"location":"api/distance/#basic-usage","title":"Basic Usage","text":"<pre><code>use vq::Distance;\n\nlet a = vec![1.0, 2.0, 3.0];\nlet b = vec![4.0, 5.0, 6.0];\n\nlet dist = Distance::Euclidean.compute(&amp;a, &amp;b)?;\nprintln!(\"Distance: {}\", dist);\n</code></pre>"},{"location":"api/distance/#all-metrics","title":"All Metrics","text":"<pre><code>use vq::Distance;\n\nlet a = vec![1.0, 0.0];\nlet b = vec![0.0, 1.0];\n\nlet metrics = [\n    (\"SquaredEuclidean\", Distance::SquaredEuclidean),\n    (\"Euclidean\", Distance::Euclidean),\n    (\"Manhattan\", Distance::Manhattan),\n    (\"CosineDistance\", Distance::CosineDistance),\n];\n\nfor (name, metric) in metrics {\n    let dist = metric.compute(&amp;a, &amp;b)?;\n    println!(\"{}: {}\", name, dist);\n}\n</code></pre>"},{"location":"api/distance/#error-handling","title":"Error Handling","text":"<pre><code>use vq::Distance;\n\nlet a = vec![1.0, 2.0];\nlet b = vec![1.0, 2.0, 3.0];\n\nmatch Distance::Euclidean.compute(&amp;a, &amp;b) {\n    Ok(dist) =&gt; println!(\"Distance: {}\", dist),\n    Err(e) =&gt; println!(\"Error: {}\", e),\n}\n</code></pre>"},{"location":"api/distance/#related-functions","title":"Related Functions","text":""},{"location":"api/distance/#get_simd_backend","title":"<code>get_simd_backend</code>","text":"<pre><code>#[cfg(feature = \"simd\")]\npub fn get_simd_backend() -&gt; String\n</code></pre> <p>Returns the name of the active SIMD backend.</p> <pre><code>#[cfg(feature = \"simd\")]\n{\n    let backend = vq::get_simd_backend();\n    println!(\"Using: {}\", backend);\n    // e.g., \"AVX2 (Auto)\", \"NEON (Auto)\", \"Scalar\"\n}\n</code></pre>"},{"location":"api/distance/#trait-implementations","title":"Trait Implementations","text":"<p><code>Distance</code> implements:</p> <ul> <li><code>Debug</code></li> <li><code>Clone</code></li> <li><code>Copy</code></li> <li><code>PartialEq</code></li> <li><code>Eq</code></li> </ul>"},{"location":"api/errors/","title":"Error Types","text":"<p>Vq uses a custom error type for all fallible operations.</p>"},{"location":"api/errors/#vqerror","title":"VqError","text":"<pre><code>pub enum VqError {\n    /// Input vector or data is empty\n    EmptyInput,\n\n    /// Vector dimensions don't match\n    DimensionMismatch { expected: usize, found: usize },\n\n    /// Invalid parameter value\n    InvalidParameter(String),\n}\n</code></pre>"},{"location":"api/errors/#variants","title":"Variants","text":""},{"location":"api/errors/#emptyinput","title":"<code>EmptyInput</code>","text":"<p>Returned when an operation receives empty input data.</p> <pre><code>let empty: Vec&lt;&amp;[f32]&gt; = vec![];\nlet result = ProductQuantizer::new(&amp;empty, 2, 4, 10, Distance::Euclidean, 0);\nassert!(matches!(result, Err(VqError::EmptyInput)));\n</code></pre>"},{"location":"api/errors/#dimensionmismatch","title":"<code>DimensionMismatch</code>","text":"<p>Returned when vector dimensions don't match expected values.</p> <pre><code>let pq = ProductQuantizer::new(&amp;training, 2, 4, 10, Distance::Euclidean, 0)?;\n// pq expects dimension 8, but we pass dimension 4\nlet result = pq.quantize(&amp;[1.0, 2.0, 3.0, 4.0]);\nassert!(matches!(result, Err(VqError::DimensionMismatch { .. })));\n</code></pre>"},{"location":"api/errors/#invalidparameter","title":"<code>InvalidParameter</code>","text":"<p>Returned when a parameter value is invalid.</p> <pre><code>// low &gt;= high is invalid\nlet result = BinaryQuantizer::new(0.0, 1, 0);\nassert!(matches!(result, Err(VqError::InvalidParameter(_))));\n\n// levels &lt; 2 is invalid\nlet result = ScalarQuantizer::new(-1.0, 1.0, 1);\nassert!(matches!(result, Err(VqError::InvalidParameter(_))));\n</code></pre>"},{"location":"api/errors/#vqresult","title":"VqResult","text":"<p>A type alias for convenience:</p> <pre><code>pub type VqResult&lt;T&gt; = Result&lt;T, VqError&gt;;\n</code></pre>"},{"location":"api/errors/#error-handling","title":"Error Handling","text":""},{"location":"api/errors/#using-operator","title":"Using <code>?</code> Operator","text":"<pre><code>use vq::{BinaryQuantizer, Quantizer, VqResult};\n\nfn process_vectors(vectors: &amp;[Vec&lt;f32&gt;]) -&gt; VqResult&lt;Vec&lt;Vec&lt;u8&gt;&gt;&gt; {\n    let bq = BinaryQuantizer::new(0.0, 0, 1)?;\n\n    vectors.iter()\n        .map(|v| bq.quantize(v))\n        .collect()\n}\n</code></pre>"},{"location":"api/errors/#using-match","title":"Using <code>match</code>","text":"<pre><code>use vq::{ScalarQuantizer, VqError};\n\nfn create_quantizer(min: f32, max: f32, levels: usize) {\n    match ScalarQuantizer::new(min, max, levels) {\n        Ok(sq) =&gt; println!(\"Created with step size: {}\", sq.step()),\n        Err(VqError::InvalidParameter(msg)) =&gt; {\n            eprintln!(\"Invalid parameter: {}\", msg);\n        }\n        Err(e) =&gt; eprintln!(\"Other error: {:?}\", e),\n    }\n}\n</code></pre>"},{"location":"api/errors/#trait-implementations","title":"Trait Implementations","text":"<p><code>VqError</code> implements:</p> <ul> <li><code>std::fmt::Debug</code></li> <li><code>std::fmt::Display</code></li> <li><code>std::error::Error</code></li> </ul> <p>This allows using Vq errors with standard Rust error handling patterns and libraries like <code>anyhow</code> or <code>thiserror</code>.</p>"},{"location":"api/quantizer/","title":"Quantizer Trait","text":"<p>The <code>Quantizer</code> trait defines the common interface for all vector quantization algorithms in Vq.</p>"},{"location":"api/quantizer/#definition","title":"Definition","text":"<pre><code>pub trait Quantizer {\n    /// The type of the quantized output\n    type QuantizedOutput;\n\n    /// Quantize a vector into a compressed representation\n    fn quantize(&amp;self, vector: &amp;[f32]) -&gt; VqResult&lt;Self::QuantizedOutput&gt;;\n\n    /// Reconstruct a vector from its quantized representation\n    fn dequantize(&amp;self, quantized: &amp;Self::QuantizedOutput) -&gt; VqResult&lt;Vec&lt;f32&gt;&gt;;\n}\n</code></pre>"},{"location":"api/quantizer/#methods","title":"Methods","text":""},{"location":"api/quantizer/#quantize","title":"<code>quantize</code>","text":"<pre><code>fn quantize(&amp;self, vector: &amp;[f32]) -&gt; VqResult&lt;Self::QuantizedOutput&gt;\n</code></pre> <p>Converts a floating-point vector into a compressed representation.</p> <ul> <li>Input: <code>&amp;[f32]</code> - the vector to quantize</li> <li>Output: <code>Self::QuantizedOutput</code> - the quantized representation</li> <li>Errors: May return <code>VqError</code> for dimension mismatches or other issues</li> </ul>"},{"location":"api/quantizer/#dequantize","title":"<code>dequantize</code>","text":"<pre><code>fn dequantize(&amp;self, quantized: &amp;Self::QuantizedOutput) -&gt; VqResult&lt;Vec&lt;f32&gt;&gt;\n</code></pre> <p>Reconstructs an approximate vector from its quantized representation.</p> <ul> <li>Input: <code>&amp;Self::QuantizedOutput</code> - the quantized data</li> <li>Output: <code>Vec&lt;f32&gt;</code> - the reconstructed vector</li> <li>Errors: May return <code>VqError</code> for invalid quantized data</li> </ul>"},{"location":"api/quantizer/#implementations","title":"Implementations","text":"Quantizer QuantizedOutput Compression <code>BinaryQuantizer</code> <code>Vec&lt;u8&gt;</code> 75% <code>ScalarQuantizer</code> <code>Vec&lt;u8&gt;</code> 75% <code>ProductQuantizer</code> <code>Vec&lt;f16&gt;</code> 50% <code>TSVQ</code> <code>Vec&lt;f16&gt;</code> 50%"},{"location":"api/quantizer/#generic-usage","title":"Generic Usage","text":"<p>The trait enables writing generic code that works with any quantizer:</p> <pre><code>use vq::{Quantizer, VqResult};\n\nfn compress_vectors&lt;Q: Quantizer&gt;(\n    quantizer: &amp;Q,\n    vectors: &amp;[Vec&lt;f32&gt;],\n) -&gt; VqResult&lt;Vec&lt;Q::QuantizedOutput&gt;&gt; {\n    vectors.iter()\n        .map(|v| quantizer.quantize(v))\n        .collect()\n}\n\nfn round_trip&lt;Q: Quantizer&gt;(\n    quantizer: &amp;Q,\n    vector: &amp;[f32],\n) -&gt; VqResult&lt;Vec&lt;f32&gt;&gt; {\n    let quantized = quantizer.quantize(vector)?;\n    quantizer.dequantize(&amp;quantized)\n}\n</code></pre>"},{"location":"api/quantizer/#example","title":"Example","text":"<pre><code>use vq::{BinaryQuantizer, ScalarQuantizer, Quantizer, VqResult};\n\nfn demonstrate_quantizer&lt;Q: Quantizer&gt;(name: &amp;str, q: &amp;Q, vector: &amp;[f32]) -&gt; VqResult&lt;()&gt;\nwhere\n    Q::QuantizedOutput: std::fmt::Debug,\n{\n    let quantized = q.quantize(vector)?;\n    let reconstructed = q.dequantize(&amp;quantized)?;\n\n    let mse: f32 = vector.iter()\n        .zip(reconstructed.iter())\n        .map(|(a, b)| (a - b).powi(2))\n        .sum::&lt;f32&gt;() / vector.len() as f32;\n\n    println!(\"{}: MSE = {:.6}\", name, mse);\n    Ok(())\n}\n\nfn main() -&gt; VqResult&lt;()&gt; {\n    let vector = vec![0.1, -0.3, 0.7, -0.9, 0.5];\n\n    let bq = BinaryQuantizer::new(0.0, 0, 1)?;\n    let sq = ScalarQuantizer::new(-1.0, 1.0, 256)?;\n\n    demonstrate_quantizer(\"BinaryQuantizer\", &amp;bq, &amp;vector)?;\n    demonstrate_quantizer(\"ScalarQuantizer\", &amp;sq, &amp;vector)?;\n\n    Ok(())\n}\n</code></pre>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>This page contains complete examples showing common Vq usage patterns.</p>"},{"location":"examples/basic/#binary-quantization","title":"Binary Quantization","text":"<pre><code>use vq::{BinaryQuantizer, Quantizer, VqResult};\n\n/// Count the number of differing bits between two binary vectors\nfn hamming_distance(a: &amp;[u8], b: &amp;[u8]) -&gt; usize {\n    a.iter().zip(b.iter()).filter(|(x, y)| x != y).count()\n}\n\nfn main() -&gt; VqResult&lt;()&gt; {\n    let bq = BinaryQuantizer::new(0.0, 0, 1)?;\n\n    // Sample embeddings\n    let embeddings = vec![\n        vec![0.5, -0.3, 0.1, -0.8, 0.2],\n        vec![0.4, -0.2, 0.0, -0.7, 0.3],  // Similar to first\n        vec![-0.6, 0.4, -0.2, 0.9, -0.1], // Different\n    ];\n\n    // Quantize all embeddings\n    let codes: Vec&lt;_&gt; = embeddings.iter()\n        .map(|e| bq.quantize(e))\n        .collect::&lt;VqResult&lt;_&gt;&gt;()?;\n\n    // Compare using Hamming distance\n    println!(\"Hamming(0, 1) = {}\", hamming_distance(&amp;codes[0], &amp;codes[1]));\n    println!(\"Hamming(0, 2) = {}\", hamming_distance(&amp;codes[0], &amp;codes[2]));\n\n    Ok(())\n}\n</code></pre>"},{"location":"examples/basic/#scalar-quantization-with-error-analysis","title":"Scalar Quantization with Error Analysis","text":"<pre><code>use vq::{ScalarQuantizer, Quantizer, VqResult};\n\nfn main() -&gt; VqResult&lt;()&gt; {\n    // Test different quantization levels\n    let levels_to_test = [4, 16, 64, 256];\n    let test_vector: Vec&lt;f32&gt; = (0..100)\n        .map(|i| (i as f32 / 50.0) - 1.0)  // Values in [-1, 1]\n        .collect();\n\n    for levels in levels_to_test {\n        let sq = ScalarQuantizer::new(-1.0, 1.0, levels)?;\n\n        let quantized = sq.quantize(&amp;test_vector)?;\n        let reconstructed = sq.dequantize(&amp;quantized)?;\n\n        let mse: f32 = test_vector.iter()\n            .zip(reconstructed.iter())\n            .map(|(a, b)| (a - b).powi(2))\n            .sum::&lt;f32&gt;() / test_vector.len() as f32;\n\n        let max_error: f32 = test_vector.iter()\n            .zip(reconstructed.iter())\n            .map(|(a, b)| (a - b).abs())\n            .fold(0.0, f32::max);\n\n        println!(\n            \"Levels: {:3} | MSE: {:.6} | Max Error: {:.4}\",\n            levels, mse, max_error\n        );\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"examples/basic/#product-quantization-for-embedding-compression","title":"Product Quantization for Embedding Compression","text":"<pre><code>use vq::{ProductQuantizer, Distance, Quantizer, VqResult};\n\nfn main() -&gt; VqResult&lt;()&gt; {\n    // Simulate 1000 embeddings of dimension 128\n    let embeddings: Vec&lt;Vec&lt;f32&gt;&gt; = (0..1000)\n        .map(|i| {\n            (0..128)\n                .map(|j| ((i * 7 + j * 13) % 1000) as f32 / 500.0 - 1.0)\n                .collect()\n        })\n        .collect();\n    let refs: Vec&lt;&amp;[f32]&gt; = embeddings.iter().map(|v| v.as_slice()).collect();\n\n    // Train PQ: 16 subspaces (128/16 = 8 dims each), 256 centroids\n    println!(\"Training PQ...\");\n    let pq = ProductQuantizer::new(&amp;refs, 16, 256, 15, Distance::SquaredEuclidean, 42)?;\n\n    println!(\"PQ Configuration:\");\n    println!(\"  Dimension: {}\", pq.dim());\n    println!(\"  Subspaces: {}\", pq.num_subspaces());\n    println!(\"  Sub-dimension: {}\", pq.sub_dim());\n\n    // Quantize and measure error\n    let mut total_mse = 0.0;\n    for emb in &amp;embeddings[..100] {\n        let quantized = pq.quantize(emb)?;\n        let reconstructed = pq.dequantize(&amp;quantized)?;\n\n        let mse: f32 = emb.iter()\n            .zip(reconstructed.iter())\n            .map(|(a, b)| (a - b).powi(2))\n            .sum::&lt;f32&gt;() / emb.len() as f32;\n        total_mse += mse;\n    }\n\n    println!(\"Average MSE: {:.6}\", total_mse / 100.0);\n\n    // Storage comparison\n    let original_bytes = 128 * 4;  // 128 floats * 4 bytes\n    let quantized_bytes = 128 * 2; // 128 f16 values * 2 bytes\n    println!(\n        \"Compression: {} bytes -&gt; {} bytes ({:.0}% reduction)\",\n        original_bytes,\n        quantized_bytes,\n        (1.0 - quantized_bytes as f64 / original_bytes as f64) * 100.0\n    );\n\n    Ok(())\n}\n</code></pre>"},{"location":"examples/basic/#distance-computation-comparison","title":"Distance Computation Comparison","text":"<pre><code>use vq::{Distance, VqResult};\n\nfn main() -&gt; VqResult&lt;()&gt; {\n    // Create test vectors\n    let a: Vec&lt;f32&gt; = (0..100).map(|i| i as f32 / 100.0).collect();\n    let b: Vec&lt;f32&gt; = (0..100).map(|i| (i as f32 / 100.0) + 0.1).collect();\n\n    // Compare all distance metrics\n    let metrics = [\n        (\"Squared Euclidean\", Distance::SquaredEuclidean),\n        (\"Euclidean\", Distance::Euclidean),\n        (\"Manhattan\", Distance::Manhattan),\n        (\"Cosine Distance\", Distance::CosineDistance),\n    ];\n\n    for (name, metric) in metrics {\n        let dist = metric.compute(&amp;a, &amp;b)?;\n        println!(\"{:20} = {:.6}\", name, dist);\n    }\n\n    // Check SIMD backend (if enabled)\n    #[cfg(feature = \"simd\")]\n    {\n        println!(\"\\nSIMD Backend: {}\", vq::get_simd_backend());\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"examples/basic/#combining-multiple-quantizers","title":"Combining Multiple Quantizers","text":"<pre><code>use vq::{BinaryQuantizer, ScalarQuantizer, Quantizer, VqResult};\n\nfn main() -&gt; VqResult&lt;()&gt; {\n    let test_vector = vec![0.1, -0.5, 0.8, -0.2, 0.6];\n\n    // Chain quantizers: first SQ, then BQ on reconstructed\n    let sq = ScalarQuantizer::new(-1.0, 1.0, 256)?;\n    let bq = BinaryQuantizer::new(0.5, 0, 1)?;\n\n    // Step 1: Scalar quantization\n    let sq_quantized = sq.quantize(&amp;test_vector)?;\n    let sq_reconstructed = sq.dequantize(&amp;sq_quantized)?;\n\n    // Step 2: Binary quantization on SQ output\n    let bq_quantized = bq.quantize(&amp;sq_reconstructed)?;\n\n    println!(\"Original: {:?}\", test_vector);\n    println!(\"After SQ: {:?}\", sq_reconstructed);\n    println!(\"After BQ: {:?}\", bq_quantized);\n\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/concepts/","title":"Vector Quantization Concepts","text":""},{"location":"getting-started/concepts/#what-is-vector-quantization","title":"What is Vector Quantization?","text":"<p>Vector quantization (VQ) is a data compression technique that represents high-dimensional vectors using a smaller set of representative values. Instead of storing full-precision floating point numbers, VQ maps vectors to compact codes that require less storage.</p>"},{"location":"getting-started/concepts/#why-use-vector-quantization","title":"Why Use Vector Quantization?","text":"<p>High-dimensional vectors (like embeddings from machine learning models) consume significant memory:</p> Vectors Dimensions Precision Storage 1M 768 float32 2.9 GB 1M 768 BQ (1-bit) 96 MB 1M 768 SQ (8-bit) 768 MB <p>Common use cases:</p> <ul> <li>Similarity search at scale (approximate nearest neighbor)</li> <li>Embedding storage for retrieval systems</li> <li>Reducing memory footprint of ML models</li> <li>Faster distance computations with compact representations</li> </ul>"},{"location":"getting-started/concepts/#compression-vs-accuracy-trade-off","title":"Compression vs. Accuracy Trade-off","text":"<p>All quantization introduces some loss of precision. The choice of algorithm depends on your accuracy requirements:</p> Algorithm Compression Reconstruction Error Training Required Binary (BQ) 75% High (binary) No Scalar (SQ) 75% Low (bounded by step size) No Product (PQ) 50% Medium (learned codebook) Yes TSVQ 50% Medium (hierarchical) Yes"},{"location":"getting-started/concepts/#algorithm-selection-guide","title":"Algorithm Selection Guide","text":""},{"location":"getting-started/concepts/#binary-quantization-bq","title":"Binary Quantization (BQ)","text":"<p>Best for: Fast approximate similarity when you only need to know if values are \"high\" or \"low\".</p> <ul> <li>Fastest quantization</li> <li>Highest compression (1 bit per value)</li> <li>No training required</li> <li>Works with Hamming distance for fast similarity</li> </ul>"},{"location":"getting-started/concepts/#scalar-quantization-sq","title":"Scalar Quantization (SQ)","text":"<p>Best for: When values fall within a known range and you need predictable reconstruction error.</p> <ul> <li>Fast quantization</li> <li>75% compression (8 bits per value)</li> <li>No training required</li> <li>Bounded reconstruction error</li> </ul>"},{"location":"getting-started/concepts/#product-quantization-pq","title":"Product Quantization (PQ)","text":"<p>Best for: Large-scale approximate nearest neighbor search.</p> <ul> <li>Divides vectors into subspaces</li> <li>Learns optimal codebooks from training data</li> <li>Good balance of compression and accuracy</li> <li>Supports efficient distance computations</li> </ul>"},{"location":"getting-started/concepts/#tree-structured-vq-tsvq","title":"Tree-Structured VQ (TSVQ)","text":"<p>Best for: Hierarchical data organization and fast quantization.</p> <ul> <li>Logarithmic quantization time</li> <li>Builds a binary tree of centroids</li> <li>Good for very high-dimensional data</li> <li>Natural hierarchical structure</li> </ul>"},{"location":"getting-started/concepts/#the-quantizer-trait","title":"The Quantizer Trait","text":"<p>All quantizers in Vq implement a common interface:</p> <pre><code>pub trait Quantizer {\n    type QuantizedOutput;\n\n    fn quantize(&amp;self, vector: &amp;[f32]) -&gt; VqResult&lt;Self::QuantizedOutput&gt;;\n    fn dequantize(&amp;self, quantized: &amp;Self::QuantizedOutput) -&gt; VqResult&lt;Vec&lt;f32&gt;&gt;;\n}\n</code></pre> <p>This allows you to swap algorithms without changing your code structure.</p>"},{"location":"getting-started/concepts/#distance-metrics","title":"Distance Metrics","text":"<p>Vq supports four distance metrics:</p> Metric Formula Use Case Euclidean \\(\\sqrt{\\sum(a_i - b_i)^2}\\) General purpose Squared Euclidean \\(\\sum(a_i - b_i)^2\\) Faster (no sqrt) Manhattan \\(\\sum\\|a_i - b_i\\|\\) Robust to outliers Cosine \\(1 - \\frac{a \\cdot b}{\\|a\\| \\|b\\|}\\) Normalized vectors <p>With the <code>simd</code> feature enabled, distance computations use SIMD instructions (AVX/NEON) for faster performance.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#rust","title":"Rust","text":"<p>Add <code>vq</code> to your project using Cargo:</p> <pre><code>cargo add vq\n</code></pre> <p>Or add it manually to your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\nvq = \"0.1\"\n</code></pre>"},{"location":"getting-started/installation/#optional-features","title":"Optional Features","text":"<p>Enable optional features for better performance:</p> <pre><code>cargo add vq --features parallel,simd\n</code></pre> Feature Description <code>parallel</code> Multi-threaded training for PQ and TSVQ <code>simd</code> SIMD-accelerated distance computations <p>SIMD Requirements</p> <p>The <code>simd</code> feature requires a C compiler supporting the C11 standard (GCC 4.9+ or Clang 3.1+). SIMD backends are automatically selected at runtime based on CPU capabilities.</p>"},{"location":"getting-started/installation/#minimum-supported-rust-version","title":"Minimum Supported Rust Version","text":"<p>Vq requires Rust 1.85 or later.</p>"},{"location":"getting-started/installation/#python","title":"Python","text":"<p>Install PyVq from PyPI:</p> <pre><code>pip install pyvq\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or later</li> <li>NumPy (automatically installed)</li> </ul>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>To build PyVq from source:</p> <pre><code>git clone https://github.com/CogitatorTech/vq.git\ncd vq/pyvq\npip install maturin\nmaturin develop --release\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide shows you how to use Vq's quantization algorithms.</p>"},{"location":"getting-started/quickstart/#binary-quantization","title":"Binary Quantization","text":"<p>Binary quantization maps values to 0 or 1 based on a threshold:</p> <pre><code>use vq::{BinaryQuantizer, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Values &gt;= 0.0 map to 1, values &lt; 0.0 map to 0\n    let bq = BinaryQuantizer::new(0.0, 0, 1)?;\n\n    let vector = vec![-0.5, 0.0, 0.5, 1.0];\n    let quantized = bq.quantize(&amp;vector)?;\n\n    println!(\"Quantized: {:?}\", quantized);\n    // Output: [0, 1, 1, 1]\n\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/quickstart/#scalar-quantization","title":"Scalar Quantization","text":"<p>Scalar quantization maps a continuous range to discrete levels:</p> <pre><code>use vq::{ScalarQuantizer, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Map values from [-1.0, 1.0] to 256 levels\n    let sq = ScalarQuantizer::new(-1.0, 1.0, 256)?;\n\n    let vector = vec![-1.0, 0.0, 0.5, 1.0];\n    let quantized = sq.quantize(&amp;vector)?;\n\n    // Reconstruct the vector\n    let reconstructed = sq.dequantize(&amp;quantized)?;\n\n    println!(\"Original:      {:?}\", vector);\n    println!(\"Reconstructed: {:?}\", reconstructed);\n\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/quickstart/#product-quantization","title":"Product Quantization","text":"<p>Product quantization requires training on a dataset:</p> <pre><code>use vq::{ProductQuantizer, Distance, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Generate training data: 100 vectors of dimension 8\n    let training: Vec&lt;Vec&lt;f32&gt;&gt; = (0..100)\n        .map(|i| (0..8).map(|j| ((i + j) % 50) as f32).collect())\n        .collect();\n    let refs: Vec&lt;&amp;[f32]&gt; = training.iter().map(|v| v.as_slice()).collect();\n\n    // Train PQ with 2 subspaces, 4 centroids each\n    let pq = ProductQuantizer::new(\n        &amp;refs,\n        2,    // m: number of subspaces\n        4,    // k: centroids per subspace\n        10,   // max iterations\n        Distance::Euclidean,\n        42,   // random seed\n    )?;\n\n    // Quantize and reconstruct\n    let quantized = pq.quantize(&amp;training[0])?;\n    let reconstructed = pq.dequantize(&amp;quantized)?;\n\n    println!(\"Dimension: {}\", pq.dim());\n    println!(\"Subspaces: {}\", pq.num_subspaces());\n\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/quickstart/#distance-computation","title":"Distance Computation","text":"<p>Compute distances between vectors:</p> <pre><code>use vq::Distance;\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    let a = vec![1.0, 2.0, 3.0];\n    let b = vec![4.0, 5.0, 6.0];\n\n    let euclidean = Distance::Euclidean.compute(&amp;a, &amp;b)?;\n    let manhattan = Distance::Manhattan.compute(&amp;a, &amp;b)?;\n    let cosine = Distance::CosineDistance.compute(&amp;a, &amp;b)?;\n\n    println!(\"Euclidean: {}\", euclidean);\n    println!(\"Manhattan: {}\", manhattan);\n    println!(\"Cosine distance: {}\", cosine);\n\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about vector quantization concepts</li> <li>Explore individual algorithms in the User Guide</li> <li>See detailed API documentation</li> </ul>"},{"location":"guide/bq/","title":"Binary Quantizer (BQ)","text":"<p>Binary quantization is the simplest form of vector quantization, mapping each value to one of two discrete levels based on a threshold comparison.</p>"},{"location":"guide/bq/#overview","title":"Overview","text":"<p>Binary quantization converts floating-point values to binary (0 or 1) based on a threshold. This achieves 75% compression by representing each value with a single bit.</p> Property Value Compression 75% Training Not required Output type <code>Vec&lt;u8&gt;</code> Complexity \\(O(nd)\\)"},{"location":"guide/bq/#creating-a-binary-quantizer","title":"Creating a Binary Quantizer","text":"<pre><code>use vq::{BinaryQuantizer, Quantizer};\n\n// Create a quantizer with threshold 0.0\n// Values &gt;= 0.0 map to 1, values &lt; 0.0 map to 0\nlet bq = BinaryQuantizer::new(\n    0.0,  // threshold\n    0,    // low value\n    1,    // high value\n)?;\n</code></pre>"},{"location":"guide/bq/#parameters","title":"Parameters","text":"Parameter Type Description <code>threshold</code> <code>f32</code> Values &gt;= threshold map to high, values &lt; threshold map to low <code>low</code> <code>u8</code> Output value for inputs below threshold <code>high</code> <code>u8</code> Output value for inputs at or above threshold"},{"location":"guide/bq/#validation","title":"Validation","text":"<p>The constructor returns an error if:</p> <ul> <li><code>low &gt;= high</code></li> <li><code>threshold</code> is NaN</li> </ul>"},{"location":"guide/bq/#quantization","title":"Quantization","text":"<pre><code>let vector = vec![-1.0, -0.5, 0.0, 0.5, 1.0];\nlet quantized = bq.quantize(&amp;vector)?;\n// Result: [0, 0, 1, 1, 1]\n</code></pre>"},{"location":"guide/bq/#dequantization","title":"Dequantization","text":"<p>Dequantization maps quantized values back to floating point:</p> <pre><code>let reconstructed = bq.dequantize(&amp;quantized)?;\n// Result: [0.0, 0.0, 1.0, 1.0, 1.0]\n</code></pre> <p>Note that dequantization does not recover the original values exactly. It maps:</p> <ul> <li>Values with <code>low</code> to 0.0</li> <li>Values with <code>high</code> to 1.0</li> </ul>"},{"location":"guide/bq/#use-cases","title":"Use Cases","text":"<p>Binary quantization is ideal for:</p> <ul> <li>Fast approximate similarity using Hamming distance</li> <li>Feature hashing</li> <li>Memory-constrained environments</li> <li>When only the sign of values matters</li> </ul>"},{"location":"guide/bq/#example-sign-based-quantization","title":"Example: Sign-Based Quantization","text":"<pre><code>use vq::{BinaryQuantizer, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Quantize based on sign (positive = 1, negative = 0)\n    let bq = BinaryQuantizer::new(0.0, 0, 1)?;\n\n    let embeddings = vec![\n        vec![-0.5, 0.3, -0.1, 0.8],\n        vec![0.2, -0.4, 0.6, -0.2],\n    ];\n\n    for emb in &amp;embeddings {\n        let codes = bq.quantize(emb)?;\n        println!(\"{:?} -&gt; {:?}\", emb, codes);\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"guide/bq/#accessor-methods","title":"Accessor Methods","text":"<pre><code>let bq = BinaryQuantizer::new(0.5, 0, 1)?;\n\nassert_eq!(bq.threshold(), 0.5);\nassert_eq!(bq.low(), 0);\nassert_eq!(bq.high(), 1);\n</code></pre>"},{"location":"guide/distance/","title":"Distance Metrics","text":"<p>Vq supports multiple distance metrics for vector comparisons. These are used both during training (PQ, TSVQ) and for finding nearest neighbors.</p>"},{"location":"guide/distance/#overview","title":"Overview","text":"<p>The <code>Distance</code> enum provides four distance metrics:</p> Metric Formula Range Use Case SquaredEuclidean \\(\\sum(a_i - b_i)^2\\) \\([0, \\infty)\\) Fast comparisons Euclidean \\(\\sqrt{\\sum(a_i - b_i)^2}\\) \\([0, \\infty)\\) General purpose Manhattan \\(\\sum\\|a_i - b_i\\|\\) \\([0, \\infty)\\) Robust to outliers CosineDistance \\(1 - \\frac{a \\cdot b}{\\|a\\| \\|b\\|}\\) \\([0, 2]\\) Normalized vectors"},{"location":"guide/distance/#using-distance","title":"Using Distance","text":"<pre><code>use vq::Distance;\n\nlet a = vec![1.0, 2.0, 3.0];\nlet b = vec![4.0, 5.0, 6.0];\n\n// Compute distances\nlet sq_euclidean = Distance::SquaredEuclidean.compute(&amp;a, &amp;b)?;\nlet euclidean = Distance::Euclidean.compute(&amp;a, &amp;b)?;\nlet manhattan = Distance::Manhattan.compute(&amp;a, &amp;b)?;\nlet cosine = Distance::CosineDistance.compute(&amp;a, &amp;b)?;\n\nprintln!(\"Squared Euclidean: {}\", sq_euclidean);  // 27.0\nprintln!(\"Euclidean: {}\", euclidean);              // ~5.196\nprintln!(\"Manhattan: {}\", manhattan);              // 9.0\nprintln!(\"Cosine distance: {}\", cosine);           // ~0.025\n</code></pre>"},{"location":"guide/distance/#metric-details","title":"Metric Details","text":""},{"location":"guide/distance/#squared-euclidean","title":"Squared Euclidean","text":"\\[d(a, b) = \\sum_{i=1}^{n} (a_i - b_i)^2\\] <ul> <li>Fastest to compute (no square root)</li> <li>Same ordering as Euclidean distance</li> <li>Use for nearest neighbor comparisons when you don't need absolute distances</li> </ul>"},{"location":"guide/distance/#euclidean","title":"Euclidean","text":"\\[d(a, b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\\] <ul> <li>Standard \"straight line\" distance</li> <li>Use when you need actual distance values</li> </ul>"},{"location":"guide/distance/#manhattan","title":"Manhattan","text":"\\[d(a, b) = \\sum_{i=1}^{n} |a_i - b_i|\\] <ul> <li>Also called L1 distance or taxicab distance</li> <li>More robust to outliers than Euclidean</li> <li>Better for sparse vectors</li> </ul>"},{"location":"guide/distance/#cosine-distance","title":"Cosine Distance","text":"\\[d(a, b) = 1 - \\frac{a \\cdot b}{\\|a\\| \\|b\\|}\\] <ul> <li>Measures angular difference, not magnitude</li> <li>Value of 0 means identical direction</li> <li>Value of 1 means orthogonal</li> <li>Value of 2 means opposite direction</li> <li>Use for normalized embeddings (e.g., sentence embeddings)</li> </ul>"},{"location":"guide/distance/#simd-acceleration","title":"SIMD Acceleration","text":"<p>When the <code>simd</code> feature is enabled, distance computations use SIMD instructions for faster performance:</p> <ul> <li>x86/x86_64: AVX, AVX2, AVX512, FMA</li> <li>ARM: NEON, SVE</li> </ul> <p>The appropriate SIMD backend is automatically selected at runtime based on CPU capabilities.</p> <pre><code>#[cfg(feature = \"simd\")]\n{\n    let backend = vq::get_simd_backend();\n    println!(\"SIMD backend: {}\", backend);\n    // e.g., \"AVX2 (Auto)\" or \"NEON (Auto)\"\n}\n</code></pre>"},{"location":"guide/distance/#error-handling","title":"Error Handling","text":"<p><code>Distance::compute()</code> returns an error if vectors have different lengths:</p> <pre><code>let a = vec![1.0, 2.0];\nlet b = vec![1.0, 2.0, 3.0];\n\nlet result = Distance::Euclidean.compute(&amp;a, &amp;b);\nassert!(result.is_err());  // DimensionMismatch error\n</code></pre>"},{"location":"guide/distance/#using-with-quantizers","title":"Using with Quantizers","text":"<p>Distance metrics are used when creating PQ and TSVQ quantizers:</p> <pre><code>use vq::{ProductQuantizer, Distance};\n\nlet pq = ProductQuantizer::new(\n    &amp;training_refs,\n    8,\n    16,\n    10,\n    Distance::Euclidean,  // Use Euclidean distance for codebook training\n    42,\n)?;\n</code></pre>"},{"location":"guide/distance/#choosing-a-distance-metric","title":"Choosing a Distance Metric","text":"Use Case Recommended Metric General nearest neighbors SquaredEuclidean (fastest) Need actual distances Euclidean Sparse vectors Manhattan Text/sentence embeddings CosineDistance Image embeddings Euclidean or CosineDistance"},{"location":"guide/pq/","title":"Product Quantizer (PQ)","text":"<p>Product quantization splits vectors into subspaces and quantizes each independently using learned codebooks, enabling efficient compression and approximate nearest neighbor search.</p>"},{"location":"guide/pq/#overview","title":"Overview","text":"<p>Product quantization divides high-dimensional vectors into smaller subspaces and learns optimal codebooks for each subspace through training. This allows for efficient distance computation using precomputed lookup tables.</p> Property Value Compression 50% Training Required Output type <code>Vec&lt;f16&gt;</code> Complexity \\(O(nd)\\) quantization, \\(O(nkd)\\) training"},{"location":"guide/pq/#creating-a-product-quantizer","title":"Creating a Product Quantizer","text":"<p>PQ requires training data to learn codebooks:</p> <pre><code>use vq::{ProductQuantizer, Distance, Quantizer};\n\n// Training data: 1000 vectors of dimension 128\nlet training: Vec&lt;Vec&lt;f32&gt;&gt; = generate_training_data();\nlet refs: Vec&lt;&amp;[f32]&gt; = training.iter().map(|v| v.as_slice()).collect();\n\n// Train PQ with 16 subspaces, 256 centroids each\nlet pq = ProductQuantizer::new(\n    &amp;refs,\n    16,     // m: number of subspaces\n    256,    // k: centroids per subspace\n    20,     // max training iterations\n    Distance::Euclidean,\n    42,     // random seed for reproducibility\n)?;\n</code></pre>"},{"location":"guide/pq/#parameters","title":"Parameters","text":"Parameter Type Description <code>training_data</code> <code>&amp;[&amp;[f32]]</code> Training vectors for learning codebooks <code>m</code> <code>usize</code> Number of subspaces to divide vectors into <code>k</code> <code>usize</code> Number of centroids per subspace <code>max_iters</code> <code>usize</code> Maximum iterations for codebook training <code>distance</code> <code>Distance</code> Distance metric to use <code>seed</code> <code>u64</code> Random seed for reproducibility"},{"location":"guide/pq/#validation","title":"Validation","text":"<p>The constructor returns an error if:</p> <ul> <li>Training data is empty</li> <li>Vector dimension is less than <code>m</code></li> <li>Vector dimension is not divisible by <code>m</code></li> </ul>"},{"location":"guide/pq/#how-pq-works","title":"How PQ Works","text":"<ol> <li>Divide each vector into <code>m</code> subvectors of dimension <code>d/m</code></li> <li>For each subspace, learn <code>k</code> centroids using LBG algorithm</li> <li>During quantization, map each subvector to its nearest centroid</li> <li>Store the centroid values (as f16) instead of centroid indices</li> </ol>"},{"location":"guide/pq/#quantization","title":"Quantization","text":"<pre><code>let vector: Vec&lt;f32&gt; = get_vector();  // dimension 128\nlet quantized = pq.quantize(&amp;vector)?;\n// Result: Vec&lt;f16&gt; of length 128 (centroid values)\n</code></pre>"},{"location":"guide/pq/#dequantization","title":"Dequantization","text":"<pre><code>let reconstructed = pq.dequantize(&amp;quantized)?;\n// Result: Vec&lt;f32&gt; of length 128\n</code></pre>"},{"location":"guide/pq/#use-cases","title":"Use Cases","text":"<p>Product quantization is ideal for:</p> <ul> <li>Large-scale approximate nearest neighbor search</li> <li>Billion-scale vector databases</li> <li>Memory-efficient embedding storage</li> <li>When training data is available</li> </ul>"},{"location":"guide/pq/#example-training-and-using-pq","title":"Example: Training and Using PQ","text":"<pre><code>use vq::{ProductQuantizer, Distance, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Generate synthetic training data\n    let training: Vec&lt;Vec&lt;f32&gt;&gt; = (0..1000)\n        .map(|i| (0..64).map(|j| ((i + j) % 100) as f32 / 100.0).collect())\n        .collect();\n    let refs: Vec&lt;&amp;[f32]&gt; = training.iter().map(|v| v.as_slice()).collect();\n\n    // Train PQ: 8 subspaces (64/8 = 8 dims each), 16 centroids each\n    let pq = ProductQuantizer::new(\n        &amp;refs,\n        8,\n        16,\n        10,\n        Distance::Euclidean,\n        0,\n    )?;\n\n    println!(\"Dimension: {}\", pq.dim());        // 64\n    println!(\"Subspaces: {}\", pq.num_subspaces()); // 8\n    println!(\"Sub-dim: {}\", pq.sub_dim());      // 8\n\n    // Quantize a vector\n    let query = &amp;training[0];\n    let quantized = pq.quantize(query)?;\n    let reconstructed = pq.dequantize(&amp;quantized)?;\n\n    // Calculate reconstruction error\n    let mse: f32 = query.iter()\n        .zip(reconstructed.iter())\n        .map(|(a, b)| (a - b).powi(2))\n        .sum::&lt;f32&gt;() / query.len() as f32;\n    println!(\"MSE: {}\", mse);\n\n    Ok(())\n}\n</code></pre>"},{"location":"guide/pq/#accessor-methods","title":"Accessor Methods","text":"<pre><code>let pq = ProductQuantizer::new(&amp;refs, 8, 16, 10, Distance::Euclidean, 0)?;\n\nassert_eq!(pq.dim(), 64);        // Expected input dimension\nassert_eq!(pq.num_subspaces(), 8); // Number of subspaces (m)\nassert_eq!(pq.sub_dim(), 8);     // Dimension per subspace (d/m)\n</code></pre>"},{"location":"guide/pq/#choosing-parameters","title":"Choosing Parameters","text":""},{"location":"guide/pq/#number-of-subspaces-m","title":"Number of Subspaces (m)","text":"<ul> <li>Higher <code>m</code> = more subspaces = faster quantization but potentially lower accuracy</li> <li>Must divide the vector dimension evenly</li> <li>Common choices: 4, 8, 16, 32</li> </ul>"},{"location":"guide/pq/#number-of-centroids-k","title":"Number of Centroids (k)","text":"<ul> <li>Higher <code>k</code> = more centroids = better accuracy but slower training</li> <li>Common choices: 16, 64, 256</li> <li>Memory per subspace: <code>k * sub_dim * sizeof(f32)</code></li> </ul>"},{"location":"guide/pq/#training-iterations","title":"Training Iterations","text":"<ul> <li>More iterations = better codebook quality</li> <li>Typically 10-20 iterations is sufficient</li> <li>Watch for convergence in reconstruction error</li> </ul>"},{"location":"guide/sq/","title":"Scalar Quantizer (SQ)","text":"<p>Scalar quantization uniformly divides a value range into discrete levels, mapping each input value to its nearest quantization level.</p>"},{"location":"guide/sq/#overview","title":"Overview","text":"<p>Scalar quantization maps floating-point values to a fixed number of discrete levels within a specified range. This achieves 75% compression by representing each value with 8 bits.</p> Property Value Compression 75% Training Not required Output type <code>Vec&lt;u8&gt;</code> Complexity \\(O(nd)\\)"},{"location":"guide/sq/#creating-a-scalar-quantizer","title":"Creating a Scalar Quantizer","text":"<pre><code>use vq::{ScalarQuantizer, Quantizer};\n\n// Create a quantizer for range [-1.0, 1.0] with 256 levels\nlet sq = ScalarQuantizer::new(\n    -1.0,  // minimum value\n    1.0,   // maximum value\n    256,   // number of levels\n)?;\n</code></pre>"},{"location":"guide/sq/#parameters","title":"Parameters","text":"Parameter Type Description <code>min</code> <code>f32</code> Minimum value in the quantization range <code>max</code> <code>f32</code> Maximum value in the quantization range <code>levels</code> <code>usize</code> Number of quantization levels (2-256)"},{"location":"guide/sq/#validation","title":"Validation","text":"<p>The constructor returns an error if:</p> <ul> <li><code>min</code> or <code>max</code> is NaN or Infinity</li> <li><code>max &lt;= min</code></li> <li><code>levels &lt; 2</code> or <code>levels &gt; 256</code></li> </ul>"},{"location":"guide/sq/#quantization","title":"Quantization","text":"<p>Values are clamped to the <code>[min, max]</code> range before quantization:</p> <pre><code>let vector = vec![-1.0, -0.5, 0.0, 0.5, 1.0];\nlet quantized = sq.quantize(&amp;vector)?;\n// Each value is mapped to a level index (0-255)\n</code></pre>"},{"location":"guide/sq/#dequantization","title":"Dequantization","text":"<p>Dequantization reconstructs approximate values from level indices:</p> <pre><code>let reconstructed = sq.dequantize(&amp;quantized)?;\n\n// Reconstruction error is bounded by step/2\nfor (orig, recon) in vector.iter().zip(reconstructed.iter()) {\n    let error = (orig - recon).abs();\n    assert!(error &lt;= sq.step() / 2.0 + 1e-6);\n}\n</code></pre>"},{"location":"guide/sq/#use-cases","title":"Use Cases","text":"<p>Scalar quantization is ideal for:</p> <ul> <li>Values with a known, bounded range</li> <li>When you need predictable reconstruction error</li> <li>Embedding compression for retrieval systems</li> <li>Gradient quantization in distributed training</li> </ul>"},{"location":"guide/sq/#example-embedding-compression","title":"Example: Embedding Compression","text":"<pre><code>use vq::{ScalarQuantizer, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Assume embeddings are normalized to [-1, 1]\n    let sq = ScalarQuantizer::new(-1.0, 1.0, 256)?;\n\n    let embedding = vec![0.1, -0.3, 0.7, -0.9, 0.0];\n\n    // Compress\n    let compressed = sq.quantize(&amp;embedding)?;\n    println!(\"Compressed: {:?}\", compressed);\n\n    // Decompress\n    let decompressed = sq.dequantize(&amp;compressed)?;\n    println!(\"Decompressed: {:?}\", decompressed);\n\n    // Calculate reconstruction error\n    let mse: f32 = embedding.iter()\n        .zip(decompressed.iter())\n        .map(|(a, b)| (a - b).powi(2))\n        .sum::&lt;f32&gt;() / embedding.len() as f32;\n    println!(\"MSE: {}\", mse);\n\n    Ok(())\n}\n</code></pre>"},{"location":"guide/sq/#accessor-methods","title":"Accessor Methods","text":"<pre><code>let sq = ScalarQuantizer::new(-1.0, 1.0, 256)?;\n\nassert_eq!(sq.min(), -1.0);\nassert_eq!(sq.max(), 1.0);\nassert_eq!(sq.levels(), 256);\nprintln!(\"Step size: {}\", sq.step());  // ~0.0078\n</code></pre>"},{"location":"guide/sq/#choosing-the-number-of-levels","title":"Choosing the Number of Levels","text":"<p>The number of levels affects both compression and accuracy:</p> Levels Bits per value Max error (for range 2.0) 2 1 1.0 4 2 0.33 16 4 0.067 256 8 0.0039 <p>For most applications, 256 levels (8-bit quantization) provides a good balance between compression and accuracy.</p>"},{"location":"guide/tsvq/","title":"Tree-Structured Vector Quantization (TSVQ)","text":"<p>Tree-structured vector quantization builds a hierarchical binary tree of cluster centroids, allowing efficient \\(O(\\log k)\\) quantization by traversing the tree to find the nearest leaf node.</p>"},{"location":"guide/tsvq/#overview","title":"Overview","text":"<p>TSVQ organizes vectors in a hierarchical tree structure. Each node contains a centroid, and the tree is split based on the dimension with maximum variance. This enables fast quantization by traversing the tree rather than comparing against all centroids.</p> Property Value Compression 50% Training Required Output type <code>Vec&lt;f16&gt;</code> Complexity \\(O(d \\log k)\\) quantization, \\(O(n \\log k)\\) training"},{"location":"guide/tsvq/#creating-a-tsvq","title":"Creating a TSVQ","text":"<p>TSVQ requires training data to build the tree:</p> <pre><code>use vq::{TSVQ, Distance, Quantizer};\n\n// Training data: 1000 vectors of dimension 64\nlet training: Vec&lt;Vec&lt;f32&gt;&gt; = generate_training_data();\nlet refs: Vec&lt;&amp;[f32]&gt; = training.iter().map(|v| v.as_slice()).collect();\n\n// Build TSVQ with max depth 6 (up to 64 leaf nodes)\nlet tsvq = TSVQ::new(\n    &amp;refs,\n    6,     // max_depth\n    Distance::Euclidean,\n)?;\n</code></pre>"},{"location":"guide/tsvq/#parameters","title":"Parameters","text":"Parameter Type Description <code>training_data</code> <code>&amp;[&amp;[f32]]</code> Training vectors for building the tree <code>max_depth</code> <code>usize</code> Maximum depth of the tree <code>distance</code> <code>Distance</code> Distance metric to use"},{"location":"guide/tsvq/#validation","title":"Validation","text":"<p>The constructor returns an error if training data is empty.</p>"},{"location":"guide/tsvq/#how-tsvq-works","title":"How TSVQ Works","text":"<ol> <li>Compute the centroid of all training vectors (root node)</li> <li>Find the dimension with maximum variance</li> <li>Split vectors at the median of that dimension</li> <li>Recursively build left and right subtrees</li> <li>Stop when reaching max depth or single vector</li> </ol> <p>During quantization, traverse the tree by comparing distances to child centroids.</p>"},{"location":"guide/tsvq/#quantization","title":"Quantization","text":"<pre><code>let vector: Vec&lt;f32&gt; = get_vector();  // dimension 64\nlet quantized = tsvq.quantize(&amp;vector)?;\n// Result: Vec&lt;f16&gt; containing the nearest leaf centroid\n</code></pre>"},{"location":"guide/tsvq/#dequantization","title":"Dequantization","text":"<pre><code>let reconstructed = tsvq.dequantize(&amp;quantized)?;\n// Result: Vec&lt;f32&gt; of the same dimension\n</code></pre>"},{"location":"guide/tsvq/#use-cases","title":"Use Cases","text":"<p>TSVQ is ideal for:</p> <ul> <li>Fast quantization of high-dimensional vectors</li> <li>Hierarchical clustering applications</li> <li>When you need sub-linear quantization time</li> <li>Adaptive precision based on tree depth</li> </ul>"},{"location":"guide/tsvq/#example-building-and-using-tsvq","title":"Example: Building and Using TSVQ","text":"<pre><code>use vq::{TSVQ, Distance, Quantizer};\n\nfn main() -&gt; vq::VqResult&lt;()&gt; {\n    // Generate synthetic training data\n    let training: Vec&lt;Vec&lt;f32&gt;&gt; = (0..500)\n        .map(|i| (0..32).map(|j| ((i * j) % 100) as f32 / 100.0).collect())\n        .collect();\n    let refs: Vec&lt;&amp;[f32]&gt; = training.iter().map(|v| v.as_slice()).collect();\n\n    // Build TSVQ with depth 5\n    let tsvq = TSVQ::new(&amp;refs, 5, Distance::SquaredEuclidean)?;\n\n    println!(\"Dimension: {}\", tsvq.dim()); // 32\n\n    // Quantize and reconstruct\n    let query = &amp;training[0];\n    let quantized = tsvq.quantize(query)?;\n    let reconstructed = tsvq.dequantize(&amp;quantized)?;\n\n    // Calculate reconstruction error\n    let mse: f32 = query.iter()\n        .zip(reconstructed.iter())\n        .map(|(a, b)| (a - b).powi(2))\n        .sum::&lt;f32&gt;() / query.len() as f32;\n    println!(\"MSE: {}\", mse);\n\n    Ok(())\n}\n</code></pre>"},{"location":"guide/tsvq/#accessor-methods","title":"Accessor Methods","text":"<pre><code>let tsvq = TSVQ::new(&amp;refs, 5, Distance::Euclidean)?;\n\nassert_eq!(tsvq.dim(), 32);  // Expected input dimension\n</code></pre>"},{"location":"guide/tsvq/#choosing-max-depth","title":"Choosing Max Depth","text":"<p>The max depth determines the maximum number of leaf nodes (up to \\(2^{\\text{depth}}\\)):</p> Depth Max Leaves Quantization Comparisons 4 16 up to 4 5 32 up to 5 6 64 up to 6 8 256 up to 8 10 1024 up to 10 <p>Deeper trees provide better accuracy but require more training data and memory.</p>"},{"location":"guide/tsvq/#comparison-with-pq","title":"Comparison with PQ","text":"Aspect TSVQ PQ Structure Hierarchical tree Flat subspaces Quantization time \\(O(d \\log k)\\) \\(O(d)\\) Training Recursive splitting LBG per subspace Accuracy Depends on tree quality Generally higher Best for Fast quantization ANN search"}]}