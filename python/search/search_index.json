{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyVq","text":"<p>PyVq provides Python bindings for the Vq vector quantization library.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>High-performance Rust implementation with Python bindings</li> <li>NumPy array support for input and output</li> <li>All quantization algorithms: BinaryQuantizer, ScalarQuantizer, ProductQuantizer, TSVQ</li> <li>SIMD-accelerated distance computations</li> <li>Simple, Pythonic API</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Binary Quantization\nbq = pyvq.BinaryQuantizer(threshold=0.0, low=0, high=1)\nvector = np.array([-0.5, 0.0, 0.5, 1.0], dtype=np.float32)\ncodes = bq.quantize(vector)\nprint(f\"Quantized: {codes}\")  # [0, 1, 1, 1]\n\n# Scalar Quantization\nsq = pyvq.ScalarQuantizer(min_val=-1.0, max_val=1.0, levels=256)\nquantized = sq.quantize(vector)\nreconstructed = sq.dequantize(quantized)\nprint(f\"Reconstructed: {reconstructed}\")\n\n# Distance Computation\ndist = pyvq.Distance.euclidean()\na = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nb = np.array([4.0, 5.0, 6.0], dtype=np.float32)\nresult = dist.compute(a, b)\nprint(f\"Euclidean distance: {result}\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyvq\n</code></pre> <p>Requires Python 3.10 or later.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>API Reference</li> <li>Examples</li> </ul>"},{"location":"#rust-library","title":"Rust Library","text":"<p>For the Rust library documentation, see docs.rs/vq or the main documentation.</p> <p>Early Development</p> <p>PyVq is in early development. Please report bugs on GitHub Issues.</p>"},{"location":"api/bq/","title":"BinaryQuantizer","text":"<p>Binary quantization maps values to 0 or 1 based on a threshold.</p>"},{"location":"api/bq/#constructor","title":"Constructor","text":"<pre><code>pyvq.BinaryQuantizer(threshold: float, low: int = 0, high: int = 1)\n</code></pre> <p>Parameters:</p> <ul> <li><code>threshold</code> - Values &gt;= threshold map to high, values &lt; threshold map to low</li> <li><code>low</code> - Output value for inputs below threshold (default: 0)</li> <li><code>high</code> - Output value for inputs at or above threshold (default: 1)</li> </ul> <p>Raises: <code>ValueError</code> if low &gt;= high or threshold is NaN</p> <p>Example:</p> <pre><code>import pyvq\n\n# Values &gt;= 0.0 become 1, values &lt; 0.0 become 0\nbq = pyvq.BinaryQuantizer(threshold=0.0, low=0, high=1)\n</code></pre>"},{"location":"api/bq/#methods","title":"Methods","text":""},{"location":"api/bq/#quantize","title":"<code>quantize</code>","text":"<pre><code>def quantize(self, vector: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Quantizes a vector to binary codes.</p> <p>Parameters:</p> <ul> <li><code>vector</code> - Input vector (numpy array of float32)</li> </ul> <p>Returns: Numpy array of uint8 containing low/high values</p> <p>Example:</p> <pre><code>import numpy as np\nimport pyvq\n\nbq = pyvq.BinaryQuantizer(0.0, 0, 1)\nvector = np.array([-1.0, 0.0, 0.5, 1.0], dtype=np.float32)\n\ncodes = bq.quantize(vector)\nprint(codes)  # [0, 1, 1, 1]\n</code></pre>"},{"location":"api/bq/#dequantize","title":"<code>dequantize</code>","text":"<pre><code>def dequantize(self, quantized: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Reconstructs a vector from binary codes.</p> <p>Parameters:</p> <ul> <li><code>quantized</code> - Quantized codes (numpy array of uint8)</li> </ul> <p>Returns: Numpy array of float32 (0.0 for low, 1.0 for high)</p> <p>Example:</p> <pre><code>codes = np.array([0, 1, 1, 1], dtype=np.uint8)\nreconstructed = bq.dequantize(codes)\nprint(reconstructed)  # [0.0, 1.0, 1.0, 1.0]\n</code></pre>"},{"location":"api/bq/#properties","title":"Properties","text":"<ul> <li><code>threshold</code> - The threshold value</li> <li><code>low</code> - The low quantization level</li> <li><code>high</code> - The high quantization level</li> </ul> <pre><code>bq = pyvq.BinaryQuantizer(0.5, 0, 1)\nprint(bq.threshold)  # 0.5\nprint(bq.low)        # 0\nprint(bq.high)       # 1\n</code></pre>"},{"location":"api/bq/#use-cases","title":"Use Cases","text":"<ul> <li>Fast Hamming distance similarity</li> <li>Sign-based feature hashing</li> <li>Memory-constrained environments</li> </ul>"},{"location":"api/distance/","title":"Distance","text":"<p>The <code>Distance</code> class provides distance metric implementations.</p>"},{"location":"api/distance/#creating-distance-objects","title":"Creating Distance Objects","text":"<pre><code>import pyvq\n\n# Factory methods\neuclidean = pyvq.Distance.euclidean()\nsquared_euclidean = pyvq.Distance.squared_euclidean()\nmanhattan = pyvq.Distance.manhattan()\ncosine = pyvq.Distance.cosine_distance()\n</code></pre>"},{"location":"api/distance/#methods","title":"Methods","text":""},{"location":"api/distance/#compute","title":"<code>compute</code>","text":"<pre><code>def compute(self, a: numpy.ndarray, b: numpy.ndarray) -&gt; float\n</code></pre> <p>Computes the distance between two vectors.</p> <p>Parameters:</p> <ul> <li><code>a</code> - First vector (numpy array of float32)</li> <li><code>b</code> - Second vector (numpy array of float32)</li> </ul> <p>Returns: Distance as a float</p> <p>Raises: <code>ValueError</code> if vectors have different lengths</p> <p>Example:</p> <pre><code>import numpy as np\nimport pyvq\n\ndist = pyvq.Distance.euclidean()\n\na = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nb = np.array([4.0, 5.0, 6.0], dtype=np.float32)\n\nresult = dist.compute(a, b)\nprint(f\"Distance: {result}\")  # ~5.196\n</code></pre>"},{"location":"api/distance/#distance-metrics","title":"Distance Metrics","text":""},{"location":"api/distance/#euclidean","title":"Euclidean","text":"\\[d(a, b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\\] <pre><code>dist = pyvq.Distance.euclidean()\n</code></pre>"},{"location":"api/distance/#squared-euclidean","title":"Squared Euclidean","text":"\\[d(a, b) = \\sum_{i=1}^{n} (a_i - b_i)^2\\] <pre><code>dist = pyvq.Distance.squared_euclidean()\n</code></pre> <p>Faster than Euclidean (no square root), same ordering for nearest neighbor.</p>"},{"location":"api/distance/#manhattan","title":"Manhattan","text":"\\[d(a, b) = \\sum_{i=1}^{n} |a_i - b_i|\\] <pre><code>dist = pyvq.Distance.manhattan()\n</code></pre> <p>Also called L1 distance. More robust to outliers.</p>"},{"location":"api/distance/#cosine-distance","title":"Cosine Distance","text":"\\[d(a, b) = 1 - \\frac{a \\cdot b}{\\|a\\| \\|b\\|}\\] <pre><code>dist = pyvq.Distance.cosine_distance()\n</code></pre> <p>Measures angular difference. Value of 0 = identical direction, 1 = orthogonal, 2 = opposite.</p>"},{"location":"api/distance/#simd-acceleration","title":"SIMD Acceleration","text":"<p>When SIMD is available (AVX/NEON), distance computations are automatically accelerated.</p> <pre><code>import pyvq\n\nbackend = pyvq.get_simd_backend()\nprint(f\"SIMD Backend: {backend}\")\n# e.g., \"AVX2 (Auto)\" or \"NEON (Auto)\"\n</code></pre>"},{"location":"api/pq/","title":"ProductQuantizer","text":"<p>Product quantization divides vectors into subspaces and learns codebooks.</p>"},{"location":"api/pq/#constructor","title":"Constructor","text":"<pre><code>pyvq.ProductQuantizer(\n    training_data: numpy.ndarray,\n    m: int,\n    k: int,\n    max_iters: int,\n    distance: pyvq.Distance,\n    seed: int\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>training_data</code> - 2D numpy array of training vectors (float32)</li> <li><code>m</code> - Number of subspaces to divide vectors into</li> <li><code>k</code> - Number of centroids per subspace</li> <li><code>max_iters</code> - Maximum iterations for codebook training</li> <li><code>distance</code> - Distance metric to use</li> <li><code>seed</code> - Random seed for reproducibility</li> </ul> <p>Raises: <code>ValueError</code> if training data is empty, dimension &lt; m, or dimension not divisible by m</p> <p>Example:</p> <pre><code>import numpy as np\nimport pyvq\n\n# Training data: 100 vectors of dimension 64\ntraining = np.random.randn(100, 64).astype(np.float32)\n\n# 8 subspaces (64/8 = 8 dims each), 256 centroids\npq = pyvq.ProductQuantizer(\n    training_data=training,\n    m=8,\n    k=256,\n    max_iters=10,\n    distance=pyvq.Distance.euclidean(),\n    seed=42\n)\n</code></pre>"},{"location":"api/pq/#methods","title":"Methods","text":""},{"location":"api/pq/#quantize","title":"<code>quantize</code>","text":"<pre><code>def quantize(self, vector: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Quantizes a vector using the learned codebooks.</p> <p>Parameters:</p> <ul> <li><code>vector</code> - Input vector (numpy array of float32, must match training dimension)</li> </ul> <p>Returns: Numpy array of float16 containing quantized centroid values</p> <p>Raises: <code>ValueError</code> if dimension doesn't match</p> <p>Example:</p> <pre><code>vector = training[0]\nquantized = pq.quantize(vector)\nprint(quantized.shape)  # (64,)\nprint(quantized.dtype)  # float16\n</code></pre>"},{"location":"api/pq/#dequantize","title":"<code>dequantize</code>","text":"<pre><code>def dequantize(self, quantized: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Reconstructs a vector from quantized representation.</p> <p>Parameters:</p> <ul> <li><code>quantized</code> - Quantized vector (numpy array of float16)</li> </ul> <p>Returns: Numpy array of float32</p> <p>Example:</p> <pre><code>reconstructed = pq.dequantize(quantized)\nmse = np.mean((vector - reconstructed) ** 2)\nprint(f\"MSE: {mse}\")\n</code></pre>"},{"location":"api/pq/#properties","title":"Properties","text":"<ul> <li><code>dim</code> - Expected input vector dimension</li> <li><code>num_subspaces</code> - Number of subspaces (m)</li> <li><code>sub_dim</code> - Dimension per subspace (dim / m)</li> </ul> <pre><code>print(pq.dim)           # 64\nprint(pq.num_subspaces) # 8\nprint(pq.sub_dim)       # 8\n</code></pre>"},{"location":"api/pq/#parameter-selection","title":"Parameter Selection","text":""},{"location":"api/pq/#number-of-subspaces-m","title":"Number of Subspaces (m)","text":"<ul> <li>Must divide the vector dimension evenly</li> <li>Higher m = faster quantization, potentially lower accuracy</li> <li>Common values: 4, 8, 16, 32</li> </ul>"},{"location":"api/pq/#number-of-centroids-k","title":"Number of Centroids (k)","text":"<ul> <li>Higher k = better accuracy, slower training</li> <li>Common values: 16, 64, 256</li> <li>Memory per subspace: k * sub_dim * 4 bytes</li> </ul>"},{"location":"api/pq/#use-cases","title":"Use Cases","text":"<ul> <li>Large-scale approximate nearest neighbor search</li> <li>Embedding compression for vector databases</li> <li>Memory-efficient storage of ML embeddings</li> </ul>"},{"location":"api/sq/","title":"ScalarQuantizer","text":"<p>Scalar quantization maps values to discrete levels within a range.</p>"},{"location":"api/sq/#constructor","title":"Constructor","text":"<pre><code>pyvq.ScalarQuantizer(min_val: float, max_val: float, levels: int)\n</code></pre> <p>Parameters:</p> <ul> <li><code>min_val</code> - Minimum value in the quantization range</li> <li><code>max_val</code> - Maximum value in the quantization range  </li> <li><code>levels</code> - Number of quantization levels (2-256)</li> </ul> <p>Raises: <code>ValueError</code> if min_val &gt;= max_val, levels &lt; 2 or &gt; 256, or values are NaN/Infinity</p> <p>Example:</p> <pre><code>import pyvq\n\n# 256 levels in range [-1, 1]\nsq = pyvq.ScalarQuantizer(min_val=-1.0, max_val=1.0, levels=256)\n</code></pre>"},{"location":"api/sq/#methods","title":"Methods","text":""},{"location":"api/sq/#quantize","title":"<code>quantize</code>","text":"<pre><code>def quantize(self, vector: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Quantizes a vector to level indices.</p> <p>Parameters:</p> <ul> <li><code>vector</code> - Input vector (numpy array of float32)</li> </ul> <p>Returns: Numpy array of uint8 containing level indices</p> <p>Values are clamped to [min_val, max_val] before quantization.</p> <p>Example:</p> <pre><code>import numpy as np\nimport pyvq\n\nsq = pyvq.ScalarQuantizer(-1.0, 1.0, 256)\nvector = np.array([-1.0, 0.0, 0.5, 1.0], dtype=np.float32)\n\nindices = sq.quantize(vector)\nprint(indices)  # [0, 127, 191, 255] (approximately)\n</code></pre>"},{"location":"api/sq/#dequantize","title":"<code>dequantize</code>","text":"<pre><code>def dequantize(self, quantized: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Reconstructs a vector from level indices.</p> <p>Parameters:</p> <ul> <li><code>quantized</code> - Level indices (numpy array of uint8)</li> </ul> <p>Returns: Numpy array of float32</p> <p>Example:</p> <pre><code>reconstructed = sq.dequantize(indices)\nprint(reconstructed)\n# Values close to original, with bounded error\n</code></pre>"},{"location":"api/sq/#properties","title":"Properties","text":"<ul> <li><code>min</code> - Minimum value</li> <li><code>max</code> - Maximum value</li> <li><code>levels</code> - Number of levels</li> <li><code>step</code> - Step size between levels</li> </ul> <pre><code>sq = pyvq.ScalarQuantizer(-1.0, 1.0, 256)\nprint(sq.min)    # -1.0\nprint(sq.max)    # 1.0\nprint(sq.levels) # 256\nprint(sq.step)   # ~0.0078\n</code></pre>"},{"location":"api/sq/#reconstruction-error","title":"Reconstruction Error","text":"<p>The maximum reconstruction error is bounded by <code>step / 2</code>:</p> <pre><code>sq = pyvq.ScalarQuantizer(-1.0, 1.0, 256)\nmax_error = sq.step / 2  # ~0.0039\n\n# Verify\nvector = np.random.uniform(-1, 1, 100).astype(np.float32)\nquantized = sq.quantize(vector)\nreconstructed = sq.dequantize(quantized)\nactual_max_error = np.max(np.abs(vector - reconstructed))\nassert actual_max_error &lt;= max_error + 1e-6\n</code></pre>"},{"location":"api/sq/#use-cases","title":"Use Cases","text":"<ul> <li>Embedding compression</li> <li>Gradient quantization</li> <li>Known-range data compression</li> </ul>"},{"location":"api/tsvq/","title":"TSVQ","text":"<p>Tree-structured vector quantization builds a hierarchical tree for fast quantization.</p>"},{"location":"api/tsvq/#constructor","title":"Constructor","text":"<pre><code>pyvq.TSVQ(\n    training_data: numpy.ndarray,\n    max_depth: int,\n    distance: pyvq.Distance\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>training_data</code> - 2D numpy array of training vectors (float32)</li> <li><code>max_depth</code> - Maximum depth of the tree</li> <li><code>distance</code> - Distance metric to use</li> </ul> <p>Raises: <code>ValueError</code> if training data is empty</p> <p>Example:</p> <pre><code>import numpy as np\nimport pyvq\n\n# Training data: 100 vectors of dimension 32\ntraining = np.random.randn(100, 32).astype(np.float32)\n\ntsvq = pyvq.TSVQ(\n    training_data=training,\n    max_depth=5,\n    distance=pyvq.Distance.squared_euclidean()\n)\n</code></pre>"},{"location":"api/tsvq/#methods","title":"Methods","text":""},{"location":"api/tsvq/#quantize","title":"<code>quantize</code>","text":"<pre><code>def quantize(self, vector: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Quantizes a vector by traversing the tree to the nearest leaf.</p> <p>Parameters:</p> <ul> <li><code>vector</code> - Input vector (numpy array of float32)</li> </ul> <p>Returns: Numpy array of float16 containing the leaf centroid</p> <p>Raises: <code>ValueError</code> if dimension doesn't match</p> <p>Example:</p> <pre><code>vector = training[0]\nquantized = tsvq.quantize(vector)\nprint(quantized.shape)  # (32,)\nprint(quantized.dtype)  # float16\n</code></pre>"},{"location":"api/tsvq/#dequantize","title":"<code>dequantize</code>","text":"<pre><code>def dequantize(self, quantized: numpy.ndarray) -&gt; numpy.ndarray\n</code></pre> <p>Reconstructs a vector from its quantized representation.</p> <p>Parameters:</p> <ul> <li><code>quantized</code> - Quantized vector (numpy array of float16)</li> </ul> <p>Returns: Numpy array of float32</p> <p>Example:</p> <pre><code>reconstructed = tsvq.dequantize(quantized)\nmse = np.mean((vector - reconstructed) ** 2)\nprint(f\"MSE: {mse}\")\n</code></pre>"},{"location":"api/tsvq/#properties","title":"Properties","text":"<ul> <li><code>dim</code> - Expected input vector dimension</li> </ul> <pre><code>print(tsvq.dim)  # 32\n</code></pre>"},{"location":"api/tsvq/#how-it-works","title":"How It Works","text":"<p>TSVQ builds a binary tree:</p> <ol> <li>Compute centroid of all training vectors (root)</li> <li>Find dimension with maximum variance</li> <li>Split vectors at median of that dimension</li> <li>Recursively build left and right subtrees</li> <li>Stop at max_depth or single vector</li> </ol> <p>Quantization traverses the tree by comparing distances to child centroids.</p>"},{"location":"api/tsvq/#parameter-selection","title":"Parameter Selection","text":""},{"location":"api/tsvq/#max-depth","title":"Max Depth","text":"<p>The depth controls the maximum number of leaf nodes (up to 2^depth):</p> Depth Max Leaves Comparisons 4 16 up to 4 5 32 up to 5 6 64 up to 6 8 256 up to 8 <p>Deeper trees = better accuracy but more memory.</p>"},{"location":"api/tsvq/#use-cases","title":"Use Cases","text":"<ul> <li>Fast quantization (logarithmic time)</li> <li>Hierarchical clustering</li> <li>When quantization speed is critical</li> </ul>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>Complete examples demonstrating PyVq usage patterns.</p>"},{"location":"examples/basic/#embedding-compression-with-scalar-quantization","title":"Embedding Compression with Scalar Quantization","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Simulate embeddings (normally from a model)\nembeddings = np.random.randn(1000, 768).astype(np.float32)\n\n# Normalize to [-1, 1] range\nembeddings = embeddings / np.abs(embeddings).max()\n\n# Create scalar quantizer\nsq = pyvq.ScalarQuantizer(min_val=-1.0, max_val=1.0, levels=256)\n\n# Compress all embeddings\ncompressed = [sq.quantize(e) for e in embeddings]\n\n# Calculate compression ratio\noriginal_bytes = embeddings.nbytes\ncompressed_bytes = sum(c.nbytes for c in compressed)\nprint(f\"Original: {original_bytes:,} bytes\")\nprint(f\"Compressed: {compressed_bytes:,} bytes\")\nprint(f\"Ratio: {original_bytes / compressed_bytes:.1f}x\")\n\n# Verify reconstruction quality\nreconstructed = np.array([sq.dequantize(c) for c in compressed])\nmse = np.mean((embeddings - reconstructed) ** 2)\nprint(f\"MSE: {mse:.6f}\")\n</code></pre>"},{"location":"examples/basic/#product-quantization-for-similarity-search","title":"Product Quantization for Similarity Search","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Create a database of vectors\ndatabase = np.random.randn(10000, 128).astype(np.float32)\n\n# Train product quantizer\npq = pyvq.ProductQuantizer(\n    training_data=database[:1000],  # Use subset for training\n    m=16,         # 16 subspaces\n    k=256,        # 256 centroids each\n    max_iters=10,\n    distance=pyvq.Distance.squared_euclidean(),\n    seed=42\n)\n\n# Quantize entire database\nquantized_db = [pq.quantize(v) for v in database]\n\n# Query - find approximate nearest neighbors\nquery = np.random.randn(128).astype(np.float32)\nquery_quantized = pq.quantize(query)\n\n# Compare distances using reconstructed vectors\ndist = pyvq.Distance.squared_euclidean()\ndistances = []\nfor i, qv in enumerate(quantized_db):\n    recon = pq.dequantize(qv)\n    d = dist.compute(query, recon)\n    distances.append((i, d))\n\n# Get top-5 nearest\nnearest = sorted(distances, key=lambda x: x[1])[:5]\nprint(\"Top 5 nearest indices:\", [n[0] for n in nearest])\n</code></pre>"},{"location":"examples/basic/#binary-hashing-for-fast-similarity","title":"Binary Hashing for Fast Similarity","text":"<pre><code>import numpy as np\nimport pyvq\n\ndef hamming_distance(a: np.ndarray, b: np.ndarray) -&gt; int:\n    \"\"\"Count differing bits between two binary vectors.\"\"\"\n    return np.sum(a != b)\n\n# Create binary quantizer\nbq = pyvq.BinaryQuantizer(threshold=0.0, low=0, high=1)\n\n# Hash some vectors\nvectors = [\n    np.array([0.5, -0.3, 0.1, -0.8, 0.2], dtype=np.float32),\n    np.array([0.4, -0.2, 0.0, -0.7, 0.3], dtype=np.float32),  # Similar\n    np.array([-0.6, 0.4, -0.2, 0.9, -0.1], dtype=np.float32), # Different\n]\n\nhashes = [bq.quantize(v) for v in vectors]\n\n# Compare using Hamming distance (fast!)\nprint(f\"Hash 0 vs 1: {hamming_distance(hashes[0], hashes[1])}\")  # Low\nprint(f\"Hash 0 vs 2: {hamming_distance(hashes[0], hashes[2])}\")  # High\n</code></pre>"},{"location":"examples/basic/#comparing-distance-metrics","title":"Comparing Distance Metrics","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Create test vectors\nnp.random.seed(42)\na = np.random.randn(100).astype(np.float32)\nb = np.random.randn(100).astype(np.float32)\n\n# All distance metrics\nmetrics = [\n    (\"Euclidean\", pyvq.Distance.euclidean()),\n    (\"Squared Euclidean\", pyvq.Distance.squared_euclidean()),\n    (\"Manhattan\", pyvq.Distance.manhattan()),\n    (\"Cosine Distance\", pyvq.Distance.cosine_distance()),\n]\n\nprint(\"Distance between random 100-d vectors:\")\nfor name, dist in metrics:\n    result = dist.compute(a, b)\n    print(f\"  {name:20s}: {result:.4f}\")\n\n# SIMD backend info\nprint(f\"\\nSIMD Backend: {pyvq.get_simd_backend()}\")\n</code></pre>"},{"location":"examples/basic/#error-analysis","title":"Error Analysis","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Test reconstruction errors for different quantizers\nvector = np.random.randn(64).astype(np.float32)\n\n# Binary Quantization\nbq = pyvq.BinaryQuantizer(0.0, 0, 1)\nbq_q = bq.quantize(vector)\nbq_r = bq.dequantize(bq_q)\nbq_mse = np.mean((vector - bq_r) ** 2)\n\n# Scalar Quantization\nsq = pyvq.ScalarQuantizer(-3.0, 3.0, 256)\nsq_q = sq.quantize(vector)\nsq_r = sq.dequantize(sq_q)\nsq_mse = np.mean((vector - sq_r) ** 2)\n\nprint(f\"Binary Quantizer MSE: {bq_mse:.4f}\")\nprint(f\"Scalar Quantizer MSE: {sq_mse:.6f}\")\n</code></pre>"},{"location":"getting-started/concepts/","title":"Vector Quantization Concepts","text":""},{"location":"getting-started/concepts/#what-is-vector-quantization","title":"What is Vector Quantization?","text":"<p>Vector quantization (VQ) is a data compression technique that represents high-dimensional vectors using a smaller set of representative values. Instead of storing full-precision floating point numbers, VQ maps vectors to compact codes that require less storage.</p>"},{"location":"getting-started/concepts/#why-use-vector-quantization","title":"Why Use Vector Quantization?","text":"<p>High-dimensional vectors consume significant memory. For example, 1 million 768-dimensional embeddings:</p> Format Storage float32 2.9 GB 8-bit (SQ) 768 MB 1-bit (BQ) 96 MB <p>Common use cases:</p> <ul> <li>Embedding storage for retrieval systems</li> <li>Approximate nearest neighbor search</li> <li>Reducing memory footprint for large vector databases</li> <li>Speeding up similarity computations</li> </ul>"},{"location":"getting-started/concepts/#compression-vs-accuracy","title":"Compression vs. Accuracy","text":"<p>All quantization introduces some loss of precision:</p> Algorithm Compression Accuracy Training Binary (BQ) 75% Low No Scalar (SQ) 75% High No Product (PQ) 50% Medium Yes TSVQ 50% Medium Yes"},{"location":"getting-started/concepts/#algorithm-overview","title":"Algorithm Overview","text":""},{"location":"getting-started/concepts/#binary-quantization","title":"Binary Quantization","text":"<p>Converts each value to 0 or 1 based on a threshold:</p> <pre><code># Values &gt;= 0 become 1, values &lt; 0 become 0\nbq = pyvq.BinaryQuantizer(threshold=0.0, low=0, high=1)\n</code></pre> <p>Best for: Fast Hamming distance comparisons, when sign is meaningful.</p>"},{"location":"getting-started/concepts/#scalar-quantization","title":"Scalar Quantization","text":"<p>Maps continuous values to discrete levels:</p> <pre><code># 256 levels in range [-1, 1]\nsq = pyvq.ScalarQuantizer(min_val=-1.0, max_val=1.0, levels=256)\n</code></pre> <p>Best for: Known value ranges, predictable reconstruction error.</p>"},{"location":"getting-started/concepts/#product-quantization","title":"Product Quantization","text":"<p>Divides vectors into subspaces and learns codebooks:</p> <pre><code>pq = pyvq.ProductQuantizer(\n    training_data=vectors,\n    m=8,      # subspaces\n    k=256,    # centroids per subspace\n    ...\n)\n</code></pre> <p>Best for: Large-scale nearest neighbor search, when training data is available.</p>"},{"location":"getting-started/concepts/#tree-structured-vq","title":"Tree-Structured VQ","text":"<p>Builds a hierarchical tree for fast quantization:</p> <pre><code>tsvq = pyvq.TSVQ(\n    training_data=vectors,\n    max_depth=6,\n    ...\n)\n</code></pre> <p>Best for: Fast quantization time, hierarchical organization.</p>"},{"location":"getting-started/concepts/#distance-metrics","title":"Distance Metrics","text":"<p>PyVq supports four distance metrics:</p> Metric Use Case Euclidean General purpose Squared Euclidean Fast comparisons (no sqrt) Manhattan Robust to outliers Cosine Distance Normalized embeddings <pre><code>dist = pyvq.Distance.euclidean()\nresult = dist.compute(vector_a, vector_b)\n</code></pre>"},{"location":"getting-started/concepts/#numpy-integration","title":"NumPy Integration","text":"<p>All PyVq functions work with NumPy arrays:</p> <ul> <li>Input vectors should be <code>np.float32</code></li> <li>Output is always <code>np.ndarray</code></li> <li>Batch processing can be done with list comprehensions</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<p>Install PyVq using pip:</p> <pre><code>pip install pyvq\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or later</li> <li>NumPy (automatically installed as a dependency)</li> </ul>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<pre><code>import pyvq\n\n# Check version\nprint(pyvq.__version__)\n\n# Check SIMD backend\nbackend = pyvq.get_simd_backend()\nprint(f\"SIMD Backend: {backend}\")\n</code></pre>"},{"location":"getting-started/installation/#building-from-source","title":"Building from Source","text":"<p>To build from source, you need:</p> <ul> <li>Rust 1.85 or later</li> <li>Python 3.10 or later</li> <li>maturin</li> </ul> <pre><code># Clone the repository\ngit clone https://github.com/CogitatorTech/vq.git\ncd vq/pyvq\n\n# Create virtual environment (recommended)\npython -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# or: .venv\\Scripts\\activate  # Windows\n\n# Install maturin\npip install maturin\n\n# Build and install in development mode\nmaturin develop --release\n</code></pre>"},{"location":"getting-started/installation/#platform-support","title":"Platform Support","text":"<p>PyVq provides pre-built wheels for:</p> <ul> <li>Linux (x86_64, aarch64)</li> <li>macOS (ARM64)</li> <li>Windows (x86_64)</li> </ul> <p>SIMD acceleration is automatically enabled on supported platforms:</p> Platform SIMD Support x86_64 AVX, AVX2, AVX512 ARM64 NEON"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide demonstrates how to use PyVq for vector quantization.</p>"},{"location":"getting-started/quickstart/#basic-quantization","title":"Basic Quantization","text":""},{"location":"getting-started/quickstart/#binary-quantization","title":"Binary Quantization","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Create a binary quantizer\n# Values &gt;= threshold map to high, values &lt; threshold map to low\nbq = pyvq.BinaryQuantizer(threshold=0.0, low=0, high=1)\n\n# Quantize a vector\nvector = np.array([-1.0, -0.5, 0.0, 0.5, 1.0], dtype=np.float32)\ncodes = bq.quantize(vector)\nprint(f\"Input:  {vector}\")\nprint(f\"Output: {codes}\")\n# Output: [0, 0, 1, 1, 1]\n</code></pre>"},{"location":"getting-started/quickstart/#scalar-quantization","title":"Scalar Quantization","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Create a scalar quantizer\n# Maps values from [-1, 1] to 256 discrete levels\nsq = pyvq.ScalarQuantizer(min_val=-1.0, max_val=1.0, levels=256)\n\n# Quantize and dequantize\nvector = np.array([0.1, -0.3, 0.7, -0.9], dtype=np.float32)\nquantized = sq.quantize(vector)\nreconstructed = sq.dequantize(quantized)\n\nprint(f\"Original:      {vector}\")\nprint(f\"Reconstructed: {reconstructed}\")\n</code></pre>"},{"location":"getting-started/quickstart/#product-quantization","title":"Product Quantization","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Generate training data: 100 vectors of dimension 16\ntraining = np.random.randn(100, 16).astype(np.float32)\n\n# Train a product quantizer\npq = pyvq.ProductQuantizer(\n    training_data=training,\n    m=4,           # 4 subspaces\n    k=8,           # 8 centroids per subspace\n    max_iters=10,\n    distance=pyvq.Distance.euclidean(),\n    seed=42\n)\n\n# Quantize a vector\nvector = training[0]\nquantized = pq.quantize(vector)\nreconstructed = pq.dequantize(quantized)\n\nprint(f\"Original dimension: {len(vector)}\")\nprint(f\"Quantized dimension: {len(quantized)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#tree-structured-vq","title":"Tree-Structured VQ","text":"<pre><code>import numpy as np\nimport pyvq\n\n# Generate training data\ntraining = np.random.randn(100, 32).astype(np.float32)\n\n# Create TSVQ with max depth 5\ntsvq = pyvq.TSVQ(\n    training_data=training,\n    max_depth=5,\n    distance=pyvq.Distance.squared_euclidean()\n)\n\n# Quantize\nvector = training[0]\nquantized = tsvq.quantize(vector)\nreconstructed = tsvq.dequantize(quantized)\n</code></pre>"},{"location":"getting-started/quickstart/#distance-computation","title":"Distance Computation","text":"<pre><code>import numpy as np\nimport pyvq\n\na = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nb = np.array([4.0, 5.0, 6.0], dtype=np.float32)\n\n# Different distance metrics\neuclidean = pyvq.Distance.euclidean()\nmanhattan = pyvq.Distance.manhattan()\ncosine = pyvq.Distance.cosine_distance()\nsq_euclidean = pyvq.Distance.squared_euclidean()\n\nprint(f\"Euclidean: {euclidean.compute(a, b)}\")\nprint(f\"Manhattan: {manhattan.compute(a, b)}\")\nprint(f\"Cosine: {cosine.compute(a, b)}\")\nprint(f\"Squared Euclidean: {sq_euclidean.compute(a, b)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-numpy-arrays","title":"Working with NumPy Arrays","text":"<p>All PyVq functions accept NumPy arrays:</p> <pre><code>import numpy as np\nimport pyvq\n\n# Input must be float32\nvectors = np.random.randn(100, 64).astype(np.float32)\n\nsq = pyvq.ScalarQuantizer(-1.0, 1.0, 256)\n\n# Quantize all vectors\nquantized = [sq.quantize(v) for v in vectors]\n\n# Results are NumPy arrays\nprint(type(quantized[0]))  # &lt;class 'numpy.ndarray'&gt;\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about vector quantization concepts</li> <li>See the API Reference</li> <li>Explore more Examples</li> </ul>"}]}